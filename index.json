[{"authors":["admin"],"categories":null,"content":"I am currently a DPhil (PhD) student at University of Oxford, AIMS CDT, advised by Prof A. Vedaldi, Dr C. Rupprecht, and Dr I. Laina at VGG. I am broadly interested in unsupervised scene understanding, with emphasis on objects.\nIn previous life, I worked as MLE for OakNorth and Bloomberg. I graduated with MEng in Computer Science from University of Cambridge, supervised by Prof Liò.\nI am always happy to discuss research, so feel free to reach out!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1689949542,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://karazijal.github.io/author/laurynas-karazija/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/laurynas-karazija/","section":"authors","summary":"I am currently a DPhil (PhD) student at University of Oxford, AIMS CDT, advised by Prof A. Vedaldi, Dr C. Rupprecht, and Dr I. Laina at VGG. I am broadly interested in unsupervised scene understanding, with emphasis on objects.","tags":null,"title":"Laurynas Karazija","type":"authors"},{"authors":["Laurynas Karazija*","Subhabrata Choudhury*","Iro Laina","Christian Rupprecht","Andrea Vedaldi"],"categories":[],"content":"","date":1665944177,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689949542,"objectID":"1580740ee3e3a99071be19779caa306a","permalink":"https://karazijal.github.io/publication/karazija2022unsupervised/","publishdate":"2022-10-13T17:15:49+01:00","relpermalink":"/publication/karazija2022unsupervised/","section":"publication","summary":"We propose a new approach to learn to segment multiple image objects without manual supervision. The method can extract objects form still images, but uses videos for supervision. While prior works have considered motion for segmentation, a key insight is that, while motion can be used to identify objects, not all objects are necessarily in motion: the absence of motion does not imply the absence of objects. Hence, our model learns to predict image regions that are likely to contain motion patterns characteristic of objects moving rigidly. It does not predict specific motion, which cannot be done unambiguously from a still image, but a distribution of possible motions, which includes the possibility that an object does not move at all. We demonstrate the advantage of this approach over its deterministic counterpart and show state-of-the-art unsupervised object segmentation performance on simulated and real-world benchmarks, surpassing methods that use motion even at test time. As our approach is applicable to variety of network architectures that segment the scenes, we also apply it to existing image reconstruction-based models showing drastic improvement.","tags":[],"title":"Unsupervised Multi-object Segmentation by Predicting Probable Motion Patterns","type":"publication"},{"authors":["Subhabrata Choudhury*","Laurynas Karazija*","Iro Laina","Andrea Vedaldi","Christian Rupprecht"],"categories":[],"content":"","date":1652724977,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689949542,"objectID":"942d3bf39361150de5fe9d93666717fa","permalink":"https://karazijal.github.io/publication/choudhury+karazija2022guess/","publishdate":"2022-10-13T17:15:49+01:00","relpermalink":"/publication/choudhury+karazija2022guess/","section":"publication","summary":"Motion, measured via optical flow, provides a powerful cue to discover and learn objects in images and videos. However, compared to using appearance, it has some blind spots, such as the fact that objects become invisible if they do not move. In this work, we propose an approach that combines the strengths of motion-based and appearance-based segmentation. We propose to supervise an image segmentation network with the pretext task of predicting regions that are likely to contain simple motion patterns, and thus likely to correspond to objects. As the model only uses a single image as input, we can apply it in two settings: unsupervised video segmentation, and unsupervised image segmentation. We achieve state-of-the-art results for videos, and demonstrate the viability of our approach on still images containing novel objects. Additionally we experiment with different motion models and optical flow backbones and find the method to be robust to these change.","tags":[],"title":"Guess What Moves: Unsupervised Video and Image Segmentation by Anticipating Motion","type":"publication"},{"authors":["Laurynas Karazija","Iro Laina","Christian Rupprecht"],"categories":[],"content":"","date":1630260977,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641245644,"objectID":"b662928ff9f93365a2d03db244d1d50f","permalink":"https://karazijal.github.io/publication/karazija2021clevrtex/","publishdate":"2022-01-02T17:15:49+01:00","relpermalink":"/publication/karazija2021clevrtex/","section":"publication","summary":"There has been a recent surge in methods that aim to decompose and segment scenes into multiple objects in an unsupervised manner, i.e., unsupervised multi-object segmentation. Performing such a task is a long-standing goal of computer vision, offering to unlock object-level reasoning without requiring dense annotations to train segmentation models. Despite significant progress, current models are developed and trained on visually simple scenes depicting mono-colored objects on plain backgrounds. The natural world, however, is visually complex with confounding aspects such as diverse textures and complicated lighting effects. In this study, we present a new benchmark called ClevrTex, designed as the next challenge to compare, evaluate and analyze algorithms. ClevrTex features synthetic scenes with diverse shapes, textures and photo-mapped materials, created using physically based rendering techniques. ClevrTex has 50k examples depicting 3-10 objects arranged on a background, created using a catalog of 60 materials, and a further test set featuring 10k images created using 25 different materials. We benchmark a large set of recent unsupervised multi-object segmentation models on ClevrTex and find all state-of-the-art approaches fail to learn good representations in the textured setting, despite impressive performance on simpler data. We also create variants of the ClevrTex dataset, controlling for different aspects of scene complexity, and probe current approaches for individual shortcomings.","tags":[],"title":"ClevrTex: A Texture-Rich Benchmark for Unsupervised Multi-Object Segmentation","type":"publication"},{"authors":["Laurynas Karazija","Petar Veličković","Pietro Liò"],"categories":[],"content":"","date":1525284977,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597600475,"objectID":"fce86809fc9e25276b3467c183255794","permalink":"https://karazijal.github.io/publication/karazija2018/","publishdate":"2020-08-16T17:15:49+01:00","relpermalink":"/publication/karazija2018/","section":"publication","summary":"This paper introduces a way to learn cross-modal convolutional neural network (X-CNN) architectures from a base convolutional network (CNN) and the training data to reduce the design cost and enable applying cross-modal networks in sparse data environments. Two approaches for building X-CNNs are presented. The base approach learns the topology in a data-driven manner, by using measurements performed on the base CNN and supplied data. The iterative approach performs further optimisation of the topology through a combined learning procedure, simultaneously learning the topology and training the network. The approaches were evaluated agains examples of hand-designed X-CNNs and their base variants, showing superior performance and, in some cases, gaining an additional 9% of accuracy. From further considerations, we conclude that the presented methodology takes less time than any manual approach would, whilst also significantly reducing the design complexity. The application of the methods is fully automated and implemented in Xsertion library.","tags":[],"title":"Automatic Inference of Cross-modal Connection Topologies for X-CNNs","type":"publication"},{"authors":["Petar Veličković","Laurynas Karazija","Nicholas D Lane","Sourav Bhattacharya","Edgar Liberis","Pietro Liò","Angela Chieh","Otmane Bellahsen","Matthieu Vegreville"],"categories":[],"content":"","date":1511937928,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597599573,"objectID":"83aca2f5215e1b51852561e8e5f52a42","permalink":"https://karazijal.github.io/publication/velickovic18cross/","publishdate":"2020-08-16T17:16:08+01:00","relpermalink":"/publication/velickovic18cross/","section":"publication","summary":"We analyse multimodal time-series data corresponding to weight, sleep and steps measurements. We focus on predicting whether a user will successfully achieve his/her weight objective. For this, we design several deep long short-term memory (LSTM) architectures, including a novel cross-modal LSTM (X-LSTM), and demonstrate their superiority over baseline approaches. The X-LSTM improves parameter efficiency by processing each modality separately and allowing for information flow between them by way of recurrent cross-connections. We present a general hyperparameter optimisation technique for X-LSTMs, which allows us to significantly improve on the LSTM and a prior state-of-the-art cross-modal approach, using a comparable number of parameters. Finally, we visualise the model's predictions, revealing implications about latent variables in this task.","tags":[],"title":"Cross-modal Recurrent Models for Weight Objective Prediction from Multimodal Time-series Data","type":"publication"}]