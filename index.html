<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Laurynas Karazija</title>
  <link rel="icon" type="image/png" href="icon.png">
  <link rel="stylesheet" href="styles.css">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
</head>
<body>
  <header>
    <div class="content">
      <h1>Laurynas Karazija</h1>
      <div class="links">
        <a href="mailto:laurynas@robots.ox.ac.uk" target="_blank"><i class="fas fa-envelope"></i></a>
        <a href="https://x.com/LKarazija" target="_blank"><i class="fa-brands fa-x-twitter"></i></a>
        <a href="https://github.com/karazijal" target="_blank"><i class="fab fa-github"></i></a>
        <a href="https://www.linkedin.com/in/laurynas-karazija-b9591b103/" target="_blank"><i class="fab fa-linkedin"></i></a>
        <a href="https://scholar.google.com/citations?user=Kyt9trwAAAAJ&hl=en" target="_blank"><i class="ai ai-google-scholar-square"></i></a>
        <a href="https://bsky.app" target="_blank"><i class="fa-brands fa-bluesky"></i></a>
      </div>
    </div>
    <button id="theme-toggle" class="theme-toggle"><i class="fa-solid fa-moon"></i></button>
  </header>
  <main>
    <section class="about">
      <div class="profile-left">
        <img src="avatar.png" alt="Profile Picture">
      </div>
      <div class="profile-right">
        <p>
          I am currently a final-year PhD student at <a href="https://www.robots.ox.ac.uk/~vgg/">VGG</a>, University of Oxford,
          advised by <a href="https://www.robots.ox.ac.uk/~vedaldi/">Prof A. Vedaldi</a>, <a href="https://chrirupp.github.io">Prof C. Rupprecht</a>, and <a href="https://scholar.google.de/citations?user=n9nXAPcAAAAJ&hl=en">Dr I. Laina</a>. My PhD is funded through <a href="https://aims.robots.ox.ac.uk">AIMS CDT</a>. This summer I interned with <a href="https://about.meta.com/uk/realitylabs/" target="_blank">Meta Reality Labs</a> <a href="https://www.projectaria.com" target="_blank">Spatial AI Systems</a> team. 
        </p>
        <p>
          In previous life, I worked as MLE for OakNorth and Bloomberg. I graduated with
          MEng in Computer Science from University of Cambridge, supervised by
          <a href="https://www.cl.cam.ac.uk/~pl219/">Prof Li√≤</a>.
        </p>
        <p>I am always happy to discuss research, so feel free to reach out!</p>
    </section>

  <section class="projects">
    <h2>Research</h2>
    <p>I'm interested in computer vision, deep learning, generative AI, and unsupervised methods and broadly how to learn about objects that comprise the world.</p>

    <div class="project highlight">
      <div class="project-content">
        <h3>Learning segmentation from point trajectories</h3>
        <p class="authors">
          <strong>Laurynas Karazija</strong>, Iro Laina, Christian Rupprecht, Andrea Vedaldi
        </p>
        <p class="venue"><em>Neural Information Processing Systems (NeurIPS) 2024</em> <span>Spotlight</span></p>
        <p class="abstract">
          We consider the problem of segmenting objects in videos based on their motion and no other forms of supervision. Prior work has often... 
          <span class="read-more" onclick="toggleAbstract(this)">[Read more]</span>
        </p>
        <div class="full-abstract" style="display: none;">
          <p>We consider the problem of segmenting objects in videos based on their motion and no other forms of supervision. Prior work has often approached this problem by using the principle of common fate, namely the fact that the motion of points that belong to the same object is strongly correlated. However, most authors have only considered instantaneous motion from optical flow. In this work, we present a way to train a segmentation network using long-term point trajectories as a supervisory signal to complement optical flow. The key difficulty is that long-term motion, unlike instantaneous motion, is difficult to model - any parametric approximation is unlikely to capture complex motion patterns over long periods of time. We instead draw inspiration from subspace clustering approaches, proposing a loss function that seeks to group the trajectories into low-rank matrices where the motion of object points can be approximately explained as a linear combination of other point tracks. Our method outperforms the prior art on motion-based segmentation, which shows the utility of long-term motion and the effectiveness of our formulation.</p>
          <span class="read-less" onclick="toggleAbstract(this)">[Show less]</span>
        </div>
        <div class="links">
          <a href="https://arxiv.org/" target="_blank"><i class="ai ai-arxiv"></i></a>
          <a href="https://www.robots.ox.ac.uk/~vgg/research/lrtl/" target="_blank"><i class="fa-solid fa-globe"></i></a>
          <a href="https://github.com/karazijal/lrtl" target="_blank"><i class="fa-brands fa-square-github"></i></a>
          <a href="https://www.youtube.com/watch?v=k2vrgPsmLFI" target="_blank"><i class="fa-solid fa-video"></i></a>
        </div>
      </div>
      <div class="project-image">
        <img src="karazija24learning/featured.png" alt="Project Image">
      </div>
    </div>

    <div class="project highlight">
      <div class="project-content">
        <h3>Diffusion Models for Open-Vocabulary Segmentation</h3>
        <p class="authors">
          <strong>Laurynas Karazija</strong>, Iro Laina, Andrea Vedaldi, Christian Rupprecht
        </p>
        <p class="venue"><em>European Conference on Computer Vision (ECCV) 2024</em> <span>Oral</span></p>
        <p class="abstract">
          Open-vocabulary segmentation is the task of segmenting anything that can be named in an image. Recently, large-scale vision-language... 
          <span class="read-more" onclick="toggleAbstract(this)">[Read more]</span>
        </p>
        <div class="full-abstract" style="display: none;">
          <p>Open-vocabulary segmentation is the task of segmenting anything that can be named in an image. Recently, large-scale vision-language modelling has led to significant advances in open-vocabulary segmentation, but at the cost of gargantuan and increasing training and annotation efforts. Hence, we ask if it is possible to use existing foundation models to synthesise on-demand efficient segmentation algorithms for specific class sets, making them applicable in an open-vocabulary setting without the need to collect further data, annotations or perform training. To that end, we present OVDiff, a novel method that leverages generative text-to-image diffusion models for unsupervised open-vocabulary segmentation. OVDiff synthesises support image sets for arbitrary textual categories, creating for each a set of prototypes representative of both the category and its surrounding context (background).It relies solely on pre-trained components and outputs the synthesised segmenter directly, without training. Our approach shows strong performance on a range of benchmarks, obtaining a lead of more than 5% over prior work on PASCAL VOC.</p>
          <span class="read-less" onclick="toggleAbstract(this)">[Show less]</span>
        </div>
        <div class="links">
          <a href="https://arxiv.org/abs/2306.09316" target="_blank"><i class="ai ai-arxiv"></i></a>
          <a href="https://www.robots.ox.ac.uk/~vgg/research/ovdiff/" target="_blank"><i class="fa-solid fa-globe"></i></a>
          <a href="https://github.com/karazijal/ovdiff" target="_blank"><i class="fa-brands fa-square-github"></i></a>
          <a href="https://youtu.be/OSDtkp7Ta-8" target="_blank"><i class="fa-solid fa-video"></i></a>
        </div>
      </div>
      <div class="project-image">
        <img src="karazija24diffusion/featured.jpg" alt="Project Image">
      </div>
    </div>

    <div class="project">
      <div class="project-content">
        <h3>Unsupervised Multi-object Segmentation by Predicting Probable Motion Patterns</h3>
        <p class="authors">
          <strong>Laurynas Karazija</strong>*, Subhabrata Choudhury*, Iro Laina, Christian Rupprecht, Andrea Vedaldi
        </p>
        <p class="venue"><em>Neural Information Processing Systems (NeurIPS) 2022</em></p>
        <p class="abstract">
          We propose a new approach to learn to segment multiple image objects without manual supervision. The method can extract objects form... 
          <span class="read-more" onclick="toggleAbstract(this)">[Read more]</span>
        </p>
        <div class="full-abstract" style="display: none;">
          <p>We propose a new approach to learn to segment multiple image objects without manual supervision. The method can extract objects form still images, but uses videos for supervision. While prior works have considered motion for segmentation, a key insight is that, while motion can be used to identify objects, not all objects are necessarily in motion: the absence of motion does not imply the absence of objects. Hence, our model learns to predict image regions that are likely to contain motion patterns characteristic of objects moving rigidly. It does not predict specific motion, which cannot be done unambiguously from a still image, but a distribution of possible motions, which includes the possibility that an object does not move at all. We demonstrate the advantage of this approach over its deterministic counterpart and show state-of-the-art unsupervised object segmentation performance on simulated and real-world benchmarks, surpassing methods that use motion even at test time. As our approach is applicable to variety of network architectures that segment the scenes, we also apply it to existing image reconstruction-based models showing drastic improvement.</p>
          <span class="read-less" onclick="toggleAbstract(this)">[Show less]</span>
        </div>
        <div class="links">
          <a href="https://arxiv.org/abs/2210.12148" target="_blank"><i class="ai ai-arxiv"></i></a>
          <a href="https://www.robots.ox.ac.uk/~vgg/research/ppmp/" target="_blank"><i class="fa-solid fa-globe"></i></a>
          <a href="https://github.com/karazijal/probable-motion" target="_blank"><i class="fa-brands fa-square-github"></i></a>
          <a href="https://slideslive.com/embed/presentation/38992430?embed_parent_url=https%3A%2F%2Fwww.robots.ox.ac.uk%2F~vgg%2Fresearch%2Fppmp%2F&embed_origin=https%3A%2F%2Fwww.robots.ox.ac.uk&embed_container_id=presentation-embed-38992430&auto_load=true&auto_play=false&zoom_ratio=&disable_fullscreen=false&locale=en&vertical_enabled=true&vertical_enabled_on_mobile=false&allow_hidden_controls_when_paused=false&fit_to_viewport=true&custom_user_id=&user_uuid=8a339d92-ccf6-46ae-a2c1-f8126010c951" target="_blank"><i class="fa-solid fa-video"></i></a>
        </div>
      </div>
      <div class="project-image">
        <img src="karazija2022unsupervised/featured.png" alt="Project Image">
      </div>
    </div>
  
    <div class="project highlight">
      <div class="project-content">
        <h3>Guess What Moves: Unsupervised Video and Image Segmentation by Anticipating Motion</h3>
        <p class="authors">
          Subhabrata Choudhury*, <strong>Laurynas Karazija</strong>*, Iro Laina, Andrea Vedaldi, Christian Rupprecht
        </p>
        <p class="venue"><em>British Machine Vision Conference (BMVC) 2022</em><span>Spotlight</span></p>
        <p class="abstract">
          Motion, measured via optical flow, provides a powerful cue to discover and learn objects in images and videos. However, compared to... 
          <span class="read-more" onclick="toggleAbstract(this)">[Read more]</span>
        </p>
        <div class="full-abstract" style="display: none;">
          <p>Motion, measured via optical flow, provides a powerful cue to discover and learn objects in images and videos. However, compared to using appearance, it has some blind spots, such as the fact that objects become invisible if they do not move. In this work, we propose an approach that combines the strengths of motion-based and appearance-based segmentation. We propose to supervise an image segmentation network with the pretext task of predicting regions that are likely to contain simple motion patterns, and thus likely to correspond to objects. As the model only uses a single image as input, we can apply it in two settings: unsupervised video segmentation, and unsupervised image segmentation. We achieve state-of-the-art results for videos, and demonstrate the viability of our approach on still images containing novel objects. Additionally we experiment with different motion models and optical flow backbones and find the method to be robust to these change.</p>
          <span class="read-less" onclick="toggleAbstract(this)">[Show less]</span>
        </div>
        <div class="links">
          <a href="https://arxiv.org/abs/2205.07844" target="_blank"><i class="ai ai-arxiv"></i></a>
          <a href="https://www.robots.ox.ac.uk/~vgg/research/gwm/" target="_blank"><i class="fa-solid fa-globe"></i></a>
          <a href="https://github.com/karazijal/guess-what-moves" target="_blank"><i class="fa-brands fa-square-github"></i></a>
          <a href="https://www.youtube.com/watch?v=wKKmRzaZmFg&feature=youtu.be" target="_blank"><i class="fa-solid fa-video"></i></a>
        </div>
      </div>
      <div class="project-image">
        <img src="choudhury+karazija2022guess/featured.png" alt="Project Image">
      </div>
    </div>

    <div class="project">
      <div class="project-content">
        <h3>ClevrTex: A Texture-Rich Benchmark for Unsupervised Multi-Object Segmentation</h3>
        <p class="authors">
          <strong>Laurynas Karazija</strong>, Iro Laina, Christian Rupprecht
        </p>
        <p class="venue"><em>Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS) 2021</em></p>
        <p class="abstract">
          There has been a recent surge in methods that aim to decompose and segment scenes into multiple objects in an unsupervised manner... 
          <span class="read-more" onclick="toggleAbstract(this)">[Read more]</span>
        </p>
        <div class="full-abstract" style="display: none;">
          <p>There has been a recent surge in methods that aim to decompose and segment scenes into multiple objects in an unsupervised manner, i.e., unsupervised multi-object segmentation. Performing such a task is a long-standing goal of computer vision, offering to unlock object-level reasoning without requiring dense annotations to train segmentation models. Despite significant progress, current models are developed and trained on visually simple scenes depicting mono-colored objects on plain backgrounds. The natural world, however, is visually complex with confounding aspects such as diverse textures and complicated lighting effects. In this study, we present a new benchmark called ClevrTex, designed as the next challenge to compare, evaluate and analyze algorithms. ClevrTex features synthetic scenes with diverse shapes, textures and photo-mapped materials, created using physically based rendering techniques. ClevrTex has 50k examples depicting 3-10 objects arranged on a background, created using a catalog of 60 materials, and a further test set featuring 10k images created using 25 different materials. We benchmark a large set of recent unsupervised multi-object segmentation models on ClevrTex and find all state-of-the-art approaches fail to learn good representations in the textured setting, despite impressive performance on simpler data. We also create variants of the ClevrTex dataset, controlling for different aspects of scene complexity, and probe current approaches for individual shortcomings.
          </p>
          <span class="read-less" onclick="toggleAbstract(this)">[Show less]</span>
        </div>
        <div class="links">
          <a href="https://arxiv.org/abs/2111.10265" target="_blank"><i class="ai ai-arxiv"></i></a>
          <a href="https://www.robots.ox.ac.uk/~vgg/data/clevrtex/" target="_blank"><i class="fa-solid fa-globe"></i></a>
          <a href="https://github.com/karazijal/clevrtex-generation" target="_blank"><i class="fa-brands fa-square-github"></i></a>
          <a href="https://slideslive.com/embed/presentation/38969525?embed_parent_url=https%3A%2F%2Fneurips.cc%2Fvirtual%2F2021%2Fdatasets-and-benchmarks%2F38449%23collapse-sl-29871&embed_container_origin=https%3A%2F%2Fneurips.cc&embed_container_id=presentation-embed-38969525" target="_blank"><i class="fa-solid fa-video"></i></a>
          <a href="karazija2021clevrtex/clevrtex_poster_fixed.png" target="_blank"><i class="fa-solid fa-image"></i></a>
        </div>
      </div>
      <div class="project-image">
        <img src="karazija2021clevrtex/featured.png" alt="Project Image">
      </div>
    </div>

    <div class="project">
      <div class="project-content">
        <h3>Automatic Inference of Cross-modal Connection Topologies for X-CNNs</h3>
        <p class="authors">
          <strong>Laurynas Karazija</strong>, Petar Veliƒçkoviƒá, Pietro Li√≤
        </p>
        <p class="venue"><em>ISNN 2018</em></p>
        <p class="abstract">
          This paper introduces a way to learn cross-modal convolutional neural network (X-CNN) architectures from a base convolutional network... 
          <span class="read-more" onclick="toggleAbstract(this)">[Read more]</span>
        </p>
        <div class="full-abstract" style="display: none;">
          <p>This paper introduces a way to learn cross-modal convolutional neural network (X-CNN) architectures from a base convolutional network (CNN) and the training data to reduce the design cost and enable applying cross-modal networks in sparse data environments. Two approaches for building X-CNNs are presented. The base approach learns the topology in a data-driven manner, by using measurements performed on the base CNN and supplied data. The iterative approach performs further optimisation of the topology through a combined learning procedure, simultaneously learning the topology and training the network. The approaches were evaluated agains examples of hand-designed X-CNNs and their base variants, showing superior performance and, in some cases, gaining an additional 9% of accuracy. From further considerations, we conclude that the presented methodology takes less time than any manual approach would, whilst also significantly reducing the design complexity. The application of the methods is fully automated and implemented in Xsertion library.
          </p>
          <span class="read-less" onclick="toggleAbstract(this)">[Show less]</span>
        </div>
        <div class="links">
          <a href="https://arxiv.org/pdf/1805.00987" target="_blank"><i class="ai ai-arxiv"></i></a>
          <a href="https://github.com/karazijal/xsertion" target="_blank"><i class="fa-brands fa-square-github"></i></a>
          <a href="karazija2018/presentation.pdf" target="_blank"><i class="fa-solid fa-image"></i></a>
        </div>
      </div>
    </div>

    <div class="project">
      <div class="project-content">
        <h3>Cross-modal Recurrent Models for Weight Objective Prediction from Multimodal Time-series Data</h3>
        <p class="authors">
          Petar Veliƒçkoviƒá, <strong>Laurynas Karazija</strong>, Nicholas D Lane, Sourav Bhattacharya, Edgar Liberis, Pietro Li√≤, Angela Chieh, Otmane Bellahsen, Matthieu Vegreville
        </p>
        <p class="venue"><em>Pervasive Health 2018</em></p>
        <p class="abstract">
          We analyse multimodal time-series data corresponding to weight, sleep and steps measurements. We focus on predicting whether a user... 
          <span class="read-more" onclick="toggleAbstract(this)">[Read more]</span>
        </p>
        <div class="full-abstract" style="display: none;">
          <p>We analyse multimodal time-series data corresponding to weight, sleep and steps measurements. We focus on predicting whether a user will successfully achieve his/her weight objective. For this, we design several deep long short-term memory (LSTM) architectures, including a novel cross-modal LSTM (X-LSTM), and demonstrate their superiority over baseline approaches. The X-LSTM improves parameter efficiency by processing each modality separately and allowing for information flow between them by way of recurrent cross-connections. We present a general hyperparameter optimisation technique for X-LSTMs, which allows us to significantly improve on the LSTM and a prior state-of-the-art cross-modal approach, using a comparable number of parameters. Finally, we visualise the model‚Äôs predictions, revealing implications about latent variables in this task.
          </p>
          <span class="read-less" onclick="toggleAbstract(this)">[Show less]</span>
        </div>
        <div class="links">
          <a href="https://dl.acm.org/doi/abs/10.1145/3240925.3240937" target="_blank"><i class="ai ai-acmdl"></i></a>
        </div>
      </div>
    </div>
  
  </section>
  <section class="services">
    <h2>Services</h2>
    <div>
      <ul>
        <li>
          <strong>Reviewer:</strong> CVPR, ICCV, ECCV, 3DV, NeurIPS (top reviewer), ICLR, IJCV.
        </li>
        <li>
          <strong>Talks:</strong>
          <ul>
            <li>
              "Unsupervised Object Learning", Aug 2024,  
              <em>Meta Surreal, Redmond, WA</em>
            </li>
            <li>
              "Segmenting Objects without Manual Supervision", Jan 2024, 
              <em>CVG, University of Bern</em>
            </li>
          </ul>
        </li>
        <li>
          <strong>Teaching Assistant:</strong>
          <ul>
            <li>Computer Vision, AIMS, University of Oxford, 2023</li>
            <li>Multi View Geometry, AIMS, University of Oxford, 2022</li>
            <li>OOP & Functional Programming, University of Cambridge, 2016</li>
          </ul>
        </li>
      </ul>
    </div>
  </section>
  <footer>
    &copy; 2025 Laurynas Karazija. All rights reserved.<br>
    This website is licensed under a <a href="https://creativecommons.org/licenses/by-sa/4.0/", target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
    <br>
    Feel free to steal <a href="https://github.com/karazijal/karazijal.github.io", target="_blank">this website</a> and make it your own. Just remember to link back.
  </footer>
  </main>
  <script>
      document.getElementById('theme-toggle').addEventListener('click', () => {
          document.body.classList.toggle('dark-mode');
          document.body.classList.contains('dark-mode')
              ? localStorage.setItem('theme', 'dark')
              : localStorage.setItem('theme', 'light');
      });

      // Apply the saved theme on load
      document.addEventListener('DOMContentLoaded', () => {
          if (localStorage.getItem('theme') === 'dark') {
              document.body.classList.add('dark-mode');
          }
      });
  </script>
  <script>
    function toggleAbstract(element) {
      const parent = element.parentNode;

      if (parent.classList.contains("abstract")) {
        // Expand to show the full abstract
        parent.style.display = "none"; // Hide the shortened abstract
        parent.nextElementSibling.style.display = "block"; // Show full abstract
        parent.nextElementSibling.scrollIntoView({ behavior: "smooth", block: "center" });
      } else {
        // Collapse back to the shortened abstract
        parent.style.display = "none"; // Hide the full abstract
        parent.previousElementSibling.style.display = "block"; // Show the shortened abstract
        parent.previousElementSibling.scrollIntoView({ behavior: "smooth", block: "center" });
      }
    }
  </script>
</body>
</html>
